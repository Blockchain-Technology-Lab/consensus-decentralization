{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Blockchain Pooling Analysis - Documentation This is the documentation for the Blockchain Pooling Analysis tool developed by the University of Edinburgh's Blockchain Technology Lab. The tool is responsible for analyzing pooling behavior of various blockchains and measuring their subsequent decentralization levels. The relevant source code is available on GitHub . Overview The tool consists of the following modules: Parser Mapping Metrics In short, the Parser is responsible for pre-processing the raw data that comes from a full node; it produces a file with all the information that is needed for the Mapping. The Mapping takes the output of the parser and combines it with some other sources of information to produce a file that reveals the distribution of resources to different entities (note that in this context \"resources\" correspond to the number of produced blocks). This distribution is the input for the Metrics module, which tracks various decentralization-related metrics and produces a file with the results. More details about the different modules can be found in the corresponding Parser , Mapping and Metrics pages. Currently, the supported ledgers are: Bitcoin Bitcoin Cash Cardano Dash Dogecoin Ethereum Litecoin Tezos Zcash We intend add more ledgers to this list in the future. Contributing This is an open source project licensed under the terms and conditions of the MIT license . Everyone is welcome to contribute to it by proposing or implementing their ideas. Example contributions include, but are not limited to, reporting potential bugs, supplying useful information for the mappings of supported ledgers, adding support for a new ledger, or making the code more efficient. Note that all contributions to the project will also be covered by the above-mentioned license. When making changes in the code, contributors are required to fork the project's repository first and then issue a pull request with their changes, which will be reviewed before being integrated to the main branch. Bugs can be reported in the Issues page, and all sorts of comments and ideas can be brought up in the project's Disccussions .","title":"Home"},{"location":"#blockchain-pooling-analysis-documentation","text":"This is the documentation for the Blockchain Pooling Analysis tool developed by the University of Edinburgh's Blockchain Technology Lab. The tool is responsible for analyzing pooling behavior of various blockchains and measuring their subsequent decentralization levels. The relevant source code is available on GitHub .","title":"Blockchain Pooling Analysis - Documentation"},{"location":"#overview","text":"The tool consists of the following modules: Parser Mapping Metrics In short, the Parser is responsible for pre-processing the raw data that comes from a full node; it produces a file with all the information that is needed for the Mapping. The Mapping takes the output of the parser and combines it with some other sources of information to produce a file that reveals the distribution of resources to different entities (note that in this context \"resources\" correspond to the number of produced blocks). This distribution is the input for the Metrics module, which tracks various decentralization-related metrics and produces a file with the results. More details about the different modules can be found in the corresponding Parser , Mapping and Metrics pages. Currently, the supported ledgers are: Bitcoin Bitcoin Cash Cardano Dash Dogecoin Ethereum Litecoin Tezos Zcash We intend add more ledgers to this list in the future.","title":"Overview"},{"location":"#contributing","text":"This is an open source project licensed under the terms and conditions of the MIT license . Everyone is welcome to contribute to it by proposing or implementing their ideas. Example contributions include, but are not limited to, reporting potential bugs, supplying useful information for the mappings of supported ledgers, adding support for a new ledger, or making the code more efficient. Note that all contributions to the project will also be covered by the above-mentioned license. When making changes in the code, contributors are required to fork the project's repository first and then issue a pull request with their changes, which will be reviewed before being integrated to the main branch. Bugs can be reported in the Issues page, and all sorts of comments and ideas can be brought up in the project's Disccussions .","title":"Contributing"},{"location":"data/","text":"Data collection Currently, the data for the analysis of the different ledgers is collected through Google BigQuery . Note that when saving results from BigQuery you should select the option \"JSONL (newline delimited)\". Sample data Bitcoin Sample raw Bitcoin data are available here . They can be retrieved using Google BigQuery with the following query: SELECT block_number as number, block_timestamp as timestamp, coinbase_param, `bigquery-public-data.crypto_bitcoin.transactions`.outputs FROM `bigquery-public-data.crypto_bitcoin.transactions` JOIN `bigquery-public-data.crypto_bitcoin.blocks` ON `bigquery-public-data.crypto_bitcoin.transactions`.block_number = `bigquery-public-data.crypto_bitcoin.blocks`.number WHERE is_coinbase is TRUE AND timestamp > '2017-12-31' Bitcoin Cash Sample raw Bitcoin Cash data are available here . They can be retrieved using Google BigQuery with the following query: SELECT block_number as number, block_timestamp as timestamp, coinbase_param, `bigquery-public-data.crypto_bitcoin_cash.transactions`.outputs FROM `bigquery-public-data.crypto_bitcoin_cash.transactions` JOIN `bigquery-public-data.crypto_bitcoin_cash.blocks` ON `bigquery-public-data.crypto_bitcoin_cash.transactions`.block_number = `bigquery-public-data.crypto_bitcoin_cash.blocks`.number WHERE is_coinbase is TRUE AND timestamp > '2018-12-31' Cardano Sample raw Cardano data are available here . They can be retrieved using Google BigQuery with the following query: SELECT `iog-data-analytics.cardano_mainnet.block`.slot_no as number, `iog-data-analytics.cardano_mainnet.pool_offline_data`.ticker_name as coinbase_param, `iog-data-analytics.cardano_mainnet.block`.block_time as timestamp, `iog-data-analytics.cardano_mainnet.block`.pool_hash FROM `iog-data-analytics.cardano_mainnet.block` LEFT JOIN `iog-data-analytics.cardano_mainnet.pool_offline_data` ON `iog-data-analytics.cardano_mainnet.block`.pool_hash = `iog-data-analytics.cardano_mainnet.pool_offline_data`.pool_hash WHERE `iog-data-analytics.cardano_mainnet.block`.block_time > '2020-12-31' Dash Sample raw Dash data are available here . They can be retrieved using Google BigQuery with the following query: SELECT block_number as number, block_timestamp as timestamp, coinbase_param, `bigquery-public-data.crypto_dash.transactions`.outputs FROM `bigquery-public-data.crypto_dash.transactions` JOIN `bigquery-public-data.crypto_dash.blocks` ON `bigquery-public-data.crypto_dash.transactions`.block_number = `bigquery-public-data.crypto_dash.blocks`.number WHERE is_coinbase is TRUE AND timestamp > '2018-12-31' Dogecoin Sample raw Dogecoin data are available here . They can be retrieved using Google BigQuery with the following query: SELECT block_number as number, block_timestamp as timestamp, coinbase_param, `bigquery-public-data.crypto_dogecoin.transactions`.outputs FROM `bigquery-public-data.crypto_dogecoin.transactions` JOIN `bigquery-public-data.crypto_dogecoin.blocks` ON `bigquery-public-data.crypto_dogecoin.transactions`.block_number = `bigquery-public-data.crypto_dogecoin.blocks`.number WHERE is_coinbase is TRUE AND timestamp > '2019-12-31' Ethereum Sample raw Ethereum data are available here . They can be retrieved using Google BigQuery with the following query: SELECT number, timestamp, miner as coinbase_addresses, extra_data as coinbase_param FROM `bigquery-public-data.crypto_ethereum.blocks` WHERE timestamp > '2018-12-31' Litecoin Sample raw Litecoin data are available here . They can be retrieved using Google BigQuery with the following query: SELECT block_number as number, block_timestamp as timestamp, coinbase_param, `bigquery-public-data.crypto_litecoin.transactions`.outputs FROM `bigquery-public-data.crypto_litecoin.transactions` JOIN `bigquery-public-data.crypto_litecoin.blocks` ON `bigquery-public-data.crypto_litecoin.transactions`.block_number = `bigquery-public-data.crypto_litecoin.blocks`.number WHERE is_coinbase is TRUE AND timestamp > '2018-12-31' Tezos Sample raw Tezos data are available here . They can be retrieved using Google BigQuery with the following query: SELECT level as number, timestamp, baker as coinbase_addresses FROM `public-data-finance.crypto_tezos.blocks` WHERE timestamp > '2020-12-31' Zcash Sample raw Zcash data are available here . They can be retrieved using Google BigQuery with the following query: SELECT block_number as number, block_timestamp as timestamp, coinbase_param, `bigquery-public-data.crypto_zcash.transactions`.outputs FROM `bigquery-public-data.crypto_zcash.transactions` JOIN `bigquery-public-data.crypto_zcash.blocks` ON `bigquery-public-data.crypto_zcash.transactions`.block_number = `bigquery-public-data.crypto_zcash.blocks`.number WHERE is_coinbase is TRUE AND timestamp > '2018-12-31'","title":"Data Collection"},{"location":"data/#data-collection","text":"Currently, the data for the analysis of the different ledgers is collected through Google BigQuery . Note that when saving results from BigQuery you should select the option \"JSONL (newline delimited)\".","title":"Data collection"},{"location":"data/#sample-data","text":"","title":"Sample data"},{"location":"data/#bitcoin","text":"Sample raw Bitcoin data are available here . They can be retrieved using Google BigQuery with the following query: SELECT block_number as number, block_timestamp as timestamp, coinbase_param, `bigquery-public-data.crypto_bitcoin.transactions`.outputs FROM `bigquery-public-data.crypto_bitcoin.transactions` JOIN `bigquery-public-data.crypto_bitcoin.blocks` ON `bigquery-public-data.crypto_bitcoin.transactions`.block_number = `bigquery-public-data.crypto_bitcoin.blocks`.number WHERE is_coinbase is TRUE AND timestamp > '2017-12-31'","title":"Bitcoin"},{"location":"data/#bitcoin-cash","text":"Sample raw Bitcoin Cash data are available here . They can be retrieved using Google BigQuery with the following query: SELECT block_number as number, block_timestamp as timestamp, coinbase_param, `bigquery-public-data.crypto_bitcoin_cash.transactions`.outputs FROM `bigquery-public-data.crypto_bitcoin_cash.transactions` JOIN `bigquery-public-data.crypto_bitcoin_cash.blocks` ON `bigquery-public-data.crypto_bitcoin_cash.transactions`.block_number = `bigquery-public-data.crypto_bitcoin_cash.blocks`.number WHERE is_coinbase is TRUE AND timestamp > '2018-12-31'","title":"Bitcoin Cash"},{"location":"data/#cardano","text":"Sample raw Cardano data are available here . They can be retrieved using Google BigQuery with the following query: SELECT `iog-data-analytics.cardano_mainnet.block`.slot_no as number, `iog-data-analytics.cardano_mainnet.pool_offline_data`.ticker_name as coinbase_param, `iog-data-analytics.cardano_mainnet.block`.block_time as timestamp, `iog-data-analytics.cardano_mainnet.block`.pool_hash FROM `iog-data-analytics.cardano_mainnet.block` LEFT JOIN `iog-data-analytics.cardano_mainnet.pool_offline_data` ON `iog-data-analytics.cardano_mainnet.block`.pool_hash = `iog-data-analytics.cardano_mainnet.pool_offline_data`.pool_hash WHERE `iog-data-analytics.cardano_mainnet.block`.block_time > '2020-12-31'","title":"Cardano"},{"location":"data/#dash","text":"Sample raw Dash data are available here . They can be retrieved using Google BigQuery with the following query: SELECT block_number as number, block_timestamp as timestamp, coinbase_param, `bigquery-public-data.crypto_dash.transactions`.outputs FROM `bigquery-public-data.crypto_dash.transactions` JOIN `bigquery-public-data.crypto_dash.blocks` ON `bigquery-public-data.crypto_dash.transactions`.block_number = `bigquery-public-data.crypto_dash.blocks`.number WHERE is_coinbase is TRUE AND timestamp > '2018-12-31'","title":"Dash"},{"location":"data/#dogecoin","text":"Sample raw Dogecoin data are available here . They can be retrieved using Google BigQuery with the following query: SELECT block_number as number, block_timestamp as timestamp, coinbase_param, `bigquery-public-data.crypto_dogecoin.transactions`.outputs FROM `bigquery-public-data.crypto_dogecoin.transactions` JOIN `bigquery-public-data.crypto_dogecoin.blocks` ON `bigquery-public-data.crypto_dogecoin.transactions`.block_number = `bigquery-public-data.crypto_dogecoin.blocks`.number WHERE is_coinbase is TRUE AND timestamp > '2019-12-31'","title":"Dogecoin"},{"location":"data/#ethereum","text":"Sample raw Ethereum data are available here . They can be retrieved using Google BigQuery with the following query: SELECT number, timestamp, miner as coinbase_addresses, extra_data as coinbase_param FROM `bigquery-public-data.crypto_ethereum.blocks` WHERE timestamp > '2018-12-31'","title":"Ethereum"},{"location":"data/#litecoin","text":"Sample raw Litecoin data are available here . They can be retrieved using Google BigQuery with the following query: SELECT block_number as number, block_timestamp as timestamp, coinbase_param, `bigquery-public-data.crypto_litecoin.transactions`.outputs FROM `bigquery-public-data.crypto_litecoin.transactions` JOIN `bigquery-public-data.crypto_litecoin.blocks` ON `bigquery-public-data.crypto_litecoin.transactions`.block_number = `bigquery-public-data.crypto_litecoin.blocks`.number WHERE is_coinbase is TRUE AND timestamp > '2018-12-31'","title":"Litecoin"},{"location":"data/#tezos","text":"Sample raw Tezos data are available here . They can be retrieved using Google BigQuery with the following query: SELECT level as number, timestamp, baker as coinbase_addresses FROM `public-data-finance.crypto_tezos.blocks` WHERE timestamp > '2020-12-31'","title":"Tezos"},{"location":"data/#zcash","text":"Sample raw Zcash data are available here . They can be retrieved using Google BigQuery with the following query: SELECT block_number as number, block_timestamp as timestamp, coinbase_param, `bigquery-public-data.crypto_zcash.transactions`.outputs FROM `bigquery-public-data.crypto_zcash.transactions` JOIN `bigquery-public-data.crypto_zcash.blocks` ON `bigquery-public-data.crypto_zcash.transactions`.block_number = `bigquery-public-data.crypto_zcash.blocks`.number WHERE is_coinbase is TRUE AND timestamp > '2018-12-31'","title":"Zcash"},{"location":"mappings/","text":"Mappings The mapping obtains the parsed data (from output/<project_name>/parsed_data.json ) and outputs a csv file that maps blocks to entities. Specifically, the csv file is structured as follows: Entity,Resources <name of entity>,<(int) number of blocks> The csv file is named as the timeframe over which the mapping was executed (e.g., 2021-04.csv ) and is stored in the project's output directory (i.e., output/<project_name> ). The logic of the mapping depends on the type of clustering we want to achieve. So, different mappings will output different results, even if applied on the same data. Pool Information To assist the mapping process, the directory helpers/pool_information contains files named <project_name>.json , with relevant pool information, structured as follows: { \"clusters\": { \"<year>\": { \"<cluster name>\": [ [\"<pool name>\", \"<source of information>\"] ] } }, \"coinbase_tags\": { \"<pool tag>\": { \"name\": \"<pool name>\", \"link\": \"<pool website>\" } }, \"pool_addresses: { \"<year>\" { \"<address>\": \"<pool name>\" } } } In this file: clusters refers to pools with shared coinbase addresses: instead of <year> , use the keyword all for clusters across years in projects other than Cardano, such links appear when an address appears in two blocks that are attributed to different pools in Cardano, such links exist when two pools share the same metadata the <source of information> should be comma-separated keywords (e.g., \"homepage\", \"shared block\", etc) or a url with the clustering information; this information should be reprodusible (a link to a community or company with information that cannot be verified independently is not acceptable) <pool tag> is the tag that a pool inserts in a block's coinbase parameter, in order to claim a block as being mined by the pool in projects that do not rely on the coinbase parameter (e.g., Cardano, Tezos) the tag is just the name of the pool Pool Ownership The file helpers/legal_links.json defines legal links between pools and companies, based on off-chain information (e.g., when a company is the major stakeholder in a pool).","title":"Mappings"},{"location":"mappings/#mappings","text":"The mapping obtains the parsed data (from output/<project_name>/parsed_data.json ) and outputs a csv file that maps blocks to entities. Specifically, the csv file is structured as follows: Entity,Resources <name of entity>,<(int) number of blocks> The csv file is named as the timeframe over which the mapping was executed (e.g., 2021-04.csv ) and is stored in the project's output directory (i.e., output/<project_name> ). The logic of the mapping depends on the type of clustering we want to achieve. So, different mappings will output different results, even if applied on the same data.","title":"Mappings"},{"location":"mappings/#pool-information","text":"To assist the mapping process, the directory helpers/pool_information contains files named <project_name>.json , with relevant pool information, structured as follows: { \"clusters\": { \"<year>\": { \"<cluster name>\": [ [\"<pool name>\", \"<source of information>\"] ] } }, \"coinbase_tags\": { \"<pool tag>\": { \"name\": \"<pool name>\", \"link\": \"<pool website>\" } }, \"pool_addresses: { \"<year>\" { \"<address>\": \"<pool name>\" } } } In this file: clusters refers to pools with shared coinbase addresses: instead of <year> , use the keyword all for clusters across years in projects other than Cardano, such links appear when an address appears in two blocks that are attributed to different pools in Cardano, such links exist when two pools share the same metadata the <source of information> should be comma-separated keywords (e.g., \"homepage\", \"shared block\", etc) or a url with the clustering information; this information should be reprodusible (a link to a community or company with information that cannot be verified independently is not acceptable) <pool tag> is the tag that a pool inserts in a block's coinbase parameter, in order to claim a block as being mined by the pool in projects that do not rely on the coinbase parameter (e.g., Cardano, Tezos) the tag is just the name of the pool","title":"Pool Information"},{"location":"mappings/#pool-ownership","text":"The file helpers/legal_links.json defines legal links between pools and companies, based on off-chain information (e.g., when a company is the major stakeholder in a pool).","title":"Pool Ownership"},{"location":"metrics/","text":"Metrics A metric gets the mapped data (see above Mapping ) and outputs a relevant value. The metrics that we currently track are the following (note that more metrics may be added in the future): Nakamoto coefficient : The Nakamoto coefficient in this context represents the minimum number of entities that collectively produce more than 50% of the system's total blocks. The output of the metric is a tuple of the Nakamoto coefficient and the power percentage that these entities (that form the coefficient) control. Gini coefficient : The Gini coefficient represents the degree of inequality when it comes to block production. The output of the metric a decimal number in [0,1], where values close to 0 are an indicator of perfect equality (all entities in the system produce the same number of blocks) and values close to 1 are an indicator of perfect inequality (one entity produces most or all blocks). Shannon entropy : The Shannon entropy represents the expected amount of information that one will gain by determining which entity produced some block. The output of the metric is a real number; typically, a high number of different involved entities will yield high entropy. Each metric is implemented in a separate Python script in the folder metrics . Each script defines a function named compute_<metric_name> , which takes as input a dictionary of the form {'<entity name>': <number of resources>} (and possibly other relevant arguments) and outputs the corresponding metric values.","title":"Metrics"},{"location":"metrics/#metrics","text":"A metric gets the mapped data (see above Mapping ) and outputs a relevant value. The metrics that we currently track are the following (note that more metrics may be added in the future): Nakamoto coefficient : The Nakamoto coefficient in this context represents the minimum number of entities that collectively produce more than 50% of the system's total blocks. The output of the metric is a tuple of the Nakamoto coefficient and the power percentage that these entities (that form the coefficient) control. Gini coefficient : The Gini coefficient represents the degree of inequality when it comes to block production. The output of the metric a decimal number in [0,1], where values close to 0 are an indicator of perfect equality (all entities in the system produce the same number of blocks) and values close to 1 are an indicator of perfect inequality (one entity produces most or all blocks). Shannon entropy : The Shannon entropy represents the expected amount of information that one will gain by determining which entity produced some block. The output of the metric is a real number; typically, a high number of different involved entities will yield high entropy. Each metric is implemented in a separate Python script in the folder metrics . Each script defines a function named compute_<metric_name> , which takes as input a dictionary of the form {'<entity name>': <number of resources>} (and possibly other relevant arguments) and outputs the corresponding metric values.","title":"Metrics"},{"location":"output/","text":"Output","title":"Output"},{"location":"output/#output","text":"","title":"Output"},{"location":"parsers/","text":"Parsers The parser obtains raw data from a full node (see Data Collection page on how to obtain the required data), parses them and outputs a json file that contains a list of entries, each entry corresponding to a block in the chain. Specifically, the input file should be placed in the input directory and named as <project_name>_raw_data.json . The output file is saved under output/<project_name>/parsed_data.json , and it is structured as follows: [ { \"number\": \"<block's number>\", \"timestamp\": \"<block's timestamp of the form: yyyy-mm-dd hh:mm:ss UTC>\", \"coinbase_addresses\": \"<address1>,<address2>\" \"coinbase_param\": \"<coinbase parameter>\" } ] While number and timestamp are consistent among different blockchains, the exact information that is included in coinbase_addresses and coinbase_param may vary. Specifically, the field coinbase_addresses corresponds to: Bitcoin , Bitcoin Cash , Dogecoin , Litecoin , Zcash , Dash : a string of comma-separated addresses which appear in the block's coinbase transaction with non-negative value (i.e., which are given some part of the block's fees) Ethereum : the miner field of the block Cardano : the hash of the pool that created the data, if defined, otherwise the empty string Tezos : the baker field of the block. And the field coinbase_param corresponds to: Bitcoin , Bitcoin Cash , Dogecoin , Litecoin , Zcash , Dash : the field coinbase_param of the block's coinbase transaction Ethereum : the field extra_data of the block Cardano : the ticker name of the pool that created the block, if defined, otherwise an empty string Tezos : there is no such field. If using BigQuery, the queries for Bitcoin, Bitcoin Cash, Dogecoin, Litecoin, Zcash, Dash return data that are parsed with the default_parser script in parsers . The query for Cardano returns data that is parsed using the cardano_parser script in parsers . All other queries return data already in the necessary parsed form, so they are parsed using a \"dummy\" parser.","title":"Parsers"},{"location":"parsers/#parsers","text":"The parser obtains raw data from a full node (see Data Collection page on how to obtain the required data), parses them and outputs a json file that contains a list of entries, each entry corresponding to a block in the chain. Specifically, the input file should be placed in the input directory and named as <project_name>_raw_data.json . The output file is saved under output/<project_name>/parsed_data.json , and it is structured as follows: [ { \"number\": \"<block's number>\", \"timestamp\": \"<block's timestamp of the form: yyyy-mm-dd hh:mm:ss UTC>\", \"coinbase_addresses\": \"<address1>,<address2>\" \"coinbase_param\": \"<coinbase parameter>\" } ] While number and timestamp are consistent among different blockchains, the exact information that is included in coinbase_addresses and coinbase_param may vary. Specifically, the field coinbase_addresses corresponds to: Bitcoin , Bitcoin Cash , Dogecoin , Litecoin , Zcash , Dash : a string of comma-separated addresses which appear in the block's coinbase transaction with non-negative value (i.e., which are given some part of the block's fees) Ethereum : the miner field of the block Cardano : the hash of the pool that created the data, if defined, otherwise the empty string Tezos : the baker field of the block. And the field coinbase_param corresponds to: Bitcoin , Bitcoin Cash , Dogecoin , Litecoin , Zcash , Dash : the field coinbase_param of the block's coinbase transaction Ethereum : the field extra_data of the block Cardano : the ticker name of the pool that created the block, if defined, otherwise an empty string Tezos : there is no such field. If using BigQuery, the queries for Bitcoin, Bitcoin Cash, Dogecoin, Litecoin, Zcash, Dash return data that are parsed with the default_parser script in parsers . The query for Cardano returns data that is parsed using the cardano_parser script in parsers . All other queries return data already in the necessary parsed form, so they are parsed using a \"dummy\" parser.","title":"Parsers"},{"location":"setup/","text":"Setup Installation To install the pooling analysis tool, simply clone this GitHub repository: git clone https://github.com/Blockchain-Technology-Lab/pooling-analysis.git The tool is written in Python 3, therefore a Python 3 interpreter is required in order to run it locally. Also take note of the requirements file , which lists the dependencies of the project, and make sure you have all of them installed before running the scripts. To install all of them in one go, you can run the following command from the root directory of the project: python -m pip install -r requirements.txt Execution The pooling analysis tool is a CLI tool, i.e. it can be executed through a terminal. In the future, a user-friendly interface may be added, but for now the best way to interact with it is by running a python script that invokes it. The run.py script in the root directory of the project invokes the required parsers, mappings and metrics, but it is also possible to execute each module individually. The following process describes the most typical workflow. Place all raw data (which could be collected from BigQuery for example, see Data Collection for more details) in the input directory, each file named as <project_name>_raw_data.json (e.g. bitcoin_raw_data.json ). By default, there is a (very small) sample input file for some supported projects; to use it, remove the prefix sample_ (but it is recommended to use larger datasets). Run python run.py --ledgers <ledger_1> <ledger_n> --timeframe <timeframe> to produce a csv of the mapped data. Note that both arguments are optional, so it's possible to omit one or both of them (in which case the default values will be used). Specifically: The ledgers argument accepts any number of the supported ledgers (case-insensitive). For example, --ledgers bitcoin would run the analysis for Bitcoin, while --ledgers Bitcoin Ethereum Cardano would run the analysis for Bitcoin, Ethereum and Cardano. If the ledgers argument is omitted, then all supported ledgers are used. The timeframe argument should be of the form YYYY-MM-DD (month and day can be omitted). For example, --timeframe 2022 would run the analysis for the year 2022, while --timeframe 2022-02 would do it for the month of February 2022. If the timeframe argument is omitted then a monthly analysis is performed for each month between January 2018 and the current month. The script will print the output of each implemented metric for the specified ledgers and timeframe. All output files can then be found under the \"Output\" directory (which is automatically created the first time the tool is run).","title":"How to use"},{"location":"setup/#setup","text":"","title":"Setup"},{"location":"setup/#installation","text":"To install the pooling analysis tool, simply clone this GitHub repository: git clone https://github.com/Blockchain-Technology-Lab/pooling-analysis.git The tool is written in Python 3, therefore a Python 3 interpreter is required in order to run it locally. Also take note of the requirements file , which lists the dependencies of the project, and make sure you have all of them installed before running the scripts. To install all of them in one go, you can run the following command from the root directory of the project: python -m pip install -r requirements.txt","title":"Installation"},{"location":"setup/#execution","text":"The pooling analysis tool is a CLI tool, i.e. it can be executed through a terminal. In the future, a user-friendly interface may be added, but for now the best way to interact with it is by running a python script that invokes it. The run.py script in the root directory of the project invokes the required parsers, mappings and metrics, but it is also possible to execute each module individually. The following process describes the most typical workflow. Place all raw data (which could be collected from BigQuery for example, see Data Collection for more details) in the input directory, each file named as <project_name>_raw_data.json (e.g. bitcoin_raw_data.json ). By default, there is a (very small) sample input file for some supported projects; to use it, remove the prefix sample_ (but it is recommended to use larger datasets). Run python run.py --ledgers <ledger_1> <ledger_n> --timeframe <timeframe> to produce a csv of the mapped data. Note that both arguments are optional, so it's possible to omit one or both of them (in which case the default values will be used). Specifically: The ledgers argument accepts any number of the supported ledgers (case-insensitive). For example, --ledgers bitcoin would run the analysis for Bitcoin, while --ledgers Bitcoin Ethereum Cardano would run the analysis for Bitcoin, Ethereum and Cardano. If the ledgers argument is omitted, then all supported ledgers are used. The timeframe argument should be of the form YYYY-MM-DD (month and day can be omitted). For example, --timeframe 2022 would run the analysis for the year 2022, while --timeframe 2022-02 would do it for the month of February 2022. If the timeframe argument is omitted then a monthly analysis is performed for each month between January 2018 and the current month. The script will print the output of each implemented metric for the specified ledgers and timeframe. All output files can then be found under the \"Output\" directory (which is automatically created the first time the tool is run).","title":"Execution"}]}